{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import workflow\n",
    "from matplotlib import pyplot as plt\n",
    "import spikeinterface as si\n",
    "from spikeinterface import widgets, exporters, postprocessing, qualitymetrics, sorters\n",
    "import probeinterface as pi\n",
    "from probeinterface import plotting\n",
    "from workflow.pipeline import *\n",
    "from workflow.utils.ingestion_utils import El2ROW\n",
    "from workflow.utils.paths import (\n",
    "    get_ephys_root_data_dir,\n",
    "    get_raw_root_data_dir,\n",
    "    get_processed_root_data_dir,\n",
    ")\n",
    "from element_interface.utils import dict_to_uuid, find_full_path, find_root_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The examples in this notebook use a sample dataset to demonstrate how to explore results. Please replace these entries with your database entries to view and analyze your data.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select from the following list of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    culture.Experiment()\n",
    "    .proj(\"experiment_end_time\", \"drug_name\", \"drug_concentration\", \"experiment_plan\")\n",
    "    .fetch(format=\"frame\")\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `spike_sorting` sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_info = dict(\n",
    "    organoid_id=\"O09\",\n",
    "    experiment_start_time=\"2023-05-18 12:25:00\",\n",
    "    insertion_number=0,\n",
    "    start_time=\"2023-05-18 12:25:00\",\n",
    "    end_time=\"2023-05-18 12:30:00\",\n",
    "    session_type=\"spike_sorting\",\n",
    ")\n",
    "\n",
    "session_probe_info = dict(\n",
    "    organoid_id=\"O09\",\n",
    "    experiment_start_time=\"2023-05-18 12:25:00\",\n",
    "    insertion_number=0,\n",
    "    start_time=\"2023-05-18 12:25:00\",\n",
    "    end_time=\"2023-05-18 12:30:00\",\n",
    "    probe=\"Q983\",  # probe serial number\n",
    "    port_id=\"A\",  # Port ID (\"A\", \"B\", etc.)\n",
    "    used_electrodes=[],  # empty if all electrodes were used\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the session\n",
    "SPIKE_SORTING_DURATION = 120  # minutes\n",
    "\n",
    "start_time = datetime.strptime(session_info[\"start_time\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "end_time = datetime.strptime(session_info[\"end_time\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "duration = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "assert (\n",
    "    session_info[\"session_type\"] == \"spike_sorting\"\n",
    "    and duration <= SPIKE_SORTING_DURATION\n",
    "), f\"Session type must be 'spike_sorting' and duration must be less than {SPIKE_SORTING_DURATION} minutes\"\n",
    "\n",
    "ephys.EphysSession.insert1(session_info, ignore_extra_fields=True, skip_duplicates=True)\n",
    "\n",
    "ephys.EphysSessionProbe.insert1(\n",
    "    session_probe_info, ignore_extra_fields=True, skip_duplicates=True\n",
    ")\n",
    "\n",
    "del session_probe_info[\"used_electrodes\"]\n",
    "display(ephys.EphysSession & session_info)\n",
    "display(ephys.EphysSessionProbe & session_probe_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = culture.Experiment().proj(\"drug_name\") * ephys.EphysSession & {\n",
    "    \"session_type\": \"spike_sorting\"\n",
    "}\n",
    "key = (query & session_info).fetch1()\n",
    "\n",
    "title = \"_\".join(\n",
    "    [\n",
    "        key[\"organoid_id\"],\n",
    "        key[\"start_time\"].strftime(\"%Y%m%d%H%M\"),\n",
    "        key[\"end_time\"].strftime(\"%Y%m%d%H%M\"),\n",
    "        key[\"drug_name\"].replace(\" \", \"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "spike_sorting_path = get_processed_root_data_dir() / \"spike_sorting\" / title\n",
    "spike_sorting_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "files, file_times = (\n",
    "    ephys.EphysRawFile\n",
    "    & f\"file_time BETWEEN '{key['start_time']}' AND '{key['end_time']}'\"\n",
    ").fetch(\"file_path\", \"file_time\", order_by=\"file_time\")\n",
    "\n",
    "[print(file) for file in files]\n",
    "print(f\"\\nNumber of files: {len(files)} ({key['drug_name']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read the raw data as a recording object.\n",
    "# 2. Concatenate the object for one session.\n",
    "\n",
    "stream_name = \"RHD2000 amplifier channel\"\n",
    "\n",
    "if (spike_sorting_path / \"recording.pkl\").exists():\n",
    "    recording = si.load_extractor(spike_sorting_path / \"recording.pkl\")\n",
    "else:\n",
    "    recording = None\n",
    "    t_start = file_times[0]\n",
    "    for file in [find_full_path(get_ephys_root_data_dir(), f) for f in files]:\n",
    "        print(f\"Processing {file}.\")\n",
    "        if not recording:\n",
    "            recording = si.extractors.read_intan(file, stream_name=stream_name)\n",
    "        else:\n",
    "            recording = si.concatenate_recordings(\n",
    "                [recording, si.extractors.read_intan(file, stream_name=stream_name)]\n",
    "            )\n",
    "\n",
    "    recording.dump_to_pickle(\n",
    "        file_path=spike_sorting_path / \"recording.pkl\"\n",
    "    )  # lazy dumping (not actual traces, only the information on how to reconstruct the recording gets dumped)\n",
    "    # recording.save(folder=spike_sorting_path)  # save on disk\n",
    "recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful APIs\n",
    "\n",
    "# traces = recording.get_traces(return_scaled=True)  # return values in uV\n",
    "# recording.get_times() # get timestamps\n",
    "# recording.get_time_info()  # {'sampling_frequency': 20000.0, 't_start': None, 'time_vector': None}\n",
    "# recording.neo_reader\n",
    "# recording.has_time_vector()  # false\n",
    "# recording.sampling_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probe info\n",
    "manufacturer = \"neuronexus\"\n",
    "probe_info = (ephys.EphysSessionProbe & key).fetch1()\n",
    "probe_type = ((probe.Probe * ephys.EphysSessionProbe()) & key).fetch1(\"probe_type\")\n",
    "\n",
    "electrode_query = probe.ElectrodeConfig.Electrode & (\n",
    "    probe.ElectrodeConfig & {\"probe_type\": probe_type}\n",
    ")\n",
    "number_of_electrodes = len(electrode_query)\n",
    "\n",
    "# Filter for used electrodes. If probe_info[\"used_electrodes\"] is None, it means all electrodes were used.\n",
    "probe_info[\"used_electrodes\"] = probe_info[\"used_electrodes\"] or list(\n",
    "    range(number_of_electrodes)\n",
    ")\n",
    "unused_electrodes = [\n",
    "    elec\n",
    "    for elec in range(number_of_electrodes)\n",
    "    if elec not in probe_info[\"used_electrodes\"]\n",
    "]\n",
    "electrode_query &= f'electrode IN {tuple(probe_info[\"used_electrodes\"])}'\n",
    "\n",
    "channel_to_electrode_map = dict(zip(*electrode_query.fetch(\"channel\", \"electrode\")))\n",
    "\n",
    "channel_to_electrode_map = {\n",
    "    f'{probe_info[\"port_id\"]}-{int(channel):03d}': electrode\n",
    "    for channel, electrode in channel_to_electrode_map.items()\n",
    "}\n",
    "print(channel_to_electrode_map)\n",
    "lfp_indices = np.sort(np.array(electrode_query.fetch(\"channel\"), dtype=int))\n",
    "\n",
    "# # Useful APIs\n",
    "# p.device_channel_indices\n",
    "# p.contact_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom plot using the probe information\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "# Create a session probe\n",
    "linear_probe = pi.generate_linear_probe(\n",
    "    num_elec=32, ypitch=100, contact_shape_params={\"radius\": 15}\n",
    ")\n",
    "linear_probe.set_device_channel_indices(El2ROW)\n",
    "\n",
    "try:\n",
    "    contact_colors = [\n",
    "        \"r\" if e in probe_info[\"used_electrodes\"] else \"w\"\n",
    "        for e in range(number_of_electrodes)\n",
    "    ]  # red for used channels\n",
    "except TypeError:\n",
    "    contact_colors = list(\"r\" * number_of_electrodes)\n",
    "\n",
    "# Plot the probe\n",
    "pi.plotting.plot_probe(linear_probe, ax=ax, contacts_colors=contact_colors)\n",
    "[spine.set_visible(False) for spine in ax.spines.values()]\n",
    "ax.yaxis.set_ticks_position(\"none\")  # Remove y-axis tick marks\n",
    "ax.set_xticks([])\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"($\\\\mu m$)\", fontsize=10)\n",
    "ax.set_title(title + \"\\n\" + probe_type)\n",
    "contact_positions = linear_probe.contact_positions\n",
    "device_channel_indices = [\n",
    "    f\"{probe_info['port_id']}-{ch:03}\" for ch in linear_probe.device_channel_indices\n",
    "]\n",
    "\n",
    "for (x, y), txt in zip(contact_positions, device_channel_indices):\n",
    "    ax.text(x + 100, y, txt, va=\"center\", fontsize=8)\n",
    "\n",
    "if not (spike_sorting_path / \"probe.pdf\").exists():\n",
    "    fig.savefig(spike_sorting_path / \"probe.pdf\")\n",
    "\n",
    "recording = recording.set_probe(linear_probe)\n",
    "recording.get_probe().to_dataframe(complete=True).sort_values(\n",
    "    by=\"contact_ids\", key=lambda col: col.astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused electrodes\n",
    "if unused_electrodes:\n",
    "    recording = recording.remove_channels(\n",
    "        remove_channel_ids=np.array([str(elec) for elec in unused_electrodes])\n",
    "    )\n",
    "print(recording)\n",
    "print(recording.get_probe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface import preprocessing\n",
    "\n",
    "recording_f = si.preprocessing.bandpass_filter(\n",
    "    recording=recording, freq_min=300, freq_max=6000\n",
    ")\n",
    "recording_cmr = si.preprocessing.common_reference(\n",
    "    recording=recording_f, operator=\"median\"\n",
    ")\n",
    "\n",
    "trace_raw = recording.get_traces(\n",
    "    start_frame=100_000, end_frame=101_000, return_scaled=True\n",
    ")\n",
    "trace_preprocessed = recording_cmr.get_traces(\n",
    "    start_frame=100_000, end_frame=101_000, return_scaled=True\n",
    ")\n",
    "\n",
    "plt.plot(trace_raw[:, 0], label=\"Raw\")\n",
    "plt.plot(trace_preprocessed[:, 0], label=\"Preprocessed\")\n",
    "plt.legend()\n",
    "\n",
    "del trace_raw, trace_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = recording_cmr.get_traces(start_frame=0, end_frame=20000, return_scaled=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ytick_loc = []\n",
    "offset = 50\n",
    "\n",
    "for i in range(data.shape[1]):\n",
    "    ax.plot(\n",
    "        np.r_[: data.shape[0]] / recording_cmr.sampling_frequency,\n",
    "        data[:, i] + i * offset,\n",
    "        linewidth=0.2,\n",
    "    )\n",
    "    ytick_loc.append(i * offset)\n",
    "\n",
    "ax.set_yticks(ytick_loc)\n",
    "ax.set_yticklabels([device_channel_indices[i] for i in probe_info[\"used_electrodes\"]])\n",
    "ax.set_title(title)\n",
    "ax.tick_params(length=0)\n",
    "ax.set(xlabel=\"Time (s)\")\n",
    "sns.despine(right=True, left=True)\n",
    "\n",
    "if not (spike_sorting_path / \"raw_trace.png\").exists():\n",
    "    fig.savefig(spike_sorting_path / \"raw_trace.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sorter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spiking circus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the following for running spiking circus\n",
    "# !pip install hdbscan\n",
    "# !pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sorter. Load the sorting data if it already exists\n",
    "sorter_name = \"spykingcircus2\"\n",
    "sorting_folder = spike_sorting_path / sorter_name\n",
    "\n",
    "if (sorting_folder / \"sorting.pkl\").exists():\n",
    "    sorting = si.load_extractor(sorting_folder / \"sorting.pkl\")\n",
    "else:\n",
    "    sorting = si.sorters.run_sorter(\n",
    "        recording=recording_cmr,\n",
    "        output_folder=sorting_folder,\n",
    "        sorter_name=sorter_name,\n",
    "        remove_existing_folder=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    sorting.dump_to_pickle(file_path=sorting_folder / \"sorting.pkl\")\n",
    "    # sorting.save(folder=sorting_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waveform extraction\n",
    "# Load if the waveform folder already exists. Otherwise, extract waveforms from the recording.\n",
    "if (sorting_folder / \"waveform\").exists():\n",
    "    we = si.load_waveforms(sorting_folder / \"waveform\", with_recording=True)\n",
    "\n",
    "else:\n",
    "    we = si.extract_waveforms(\n",
    "        recording_cmr,\n",
    "        sorting,\n",
    "        folder=sorting_folder / \"waveform\",\n",
    "        ms_before=1.5,\n",
    "        ms_after=2.0,\n",
    "        max_spikes_per_unit=500,\n",
    "        # overwrite=True,\n",
    "    )\n",
    "    print(we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(we.get_template(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.widgets.plot_unit_templates(we, unit_ids=sorting.unit_ids[:5], ncols=5)\n",
    "# si.widgets.plot_unit_templates(we, ncols=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_id = 34\n",
    "si.widgets.plot_unit_waveforms(we, unit_ids=[sorting.unit_ids[unit_id]])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "template = we.get_template(unit_id=sorting.unit_ids[unit_id], mode=\"median\")\n",
    "ax.plot(template[:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rasters\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 15))\n",
    "si.widgets.plot_rasters(sorting, time_range=[0, 5], ax=ax)\n",
    "ax.set_ylabel(\"Unit ID\")\n",
    "ax.set_title(title)\n",
    "# si.widgets.plot_rasters(sorting, time_range=[0, 5], unit_ids=[unit_id], ax=ax)\n",
    "sns.despine()\n",
    "\n",
    "if not (spike_sorting_path / \"raster.png\").exists():\n",
    "    fig.savefig(spike_sorting_path / \"raster.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save quality metrics\n",
    "metrics = si.qualitymetrics.compute_quality_metrics(\n",
    "    we,\n",
    "    metric_names=[\n",
    "        \"firing_rate\",\n",
    "        \"snr\",\n",
    "        \"presence_ratio\",\n",
    "        \"isi_violation\",\n",
    "        \"num_spikes\",\n",
    "        \"amplitude_cutoff\",\n",
    "        \"amplitude_median\",\n",
    "        \"sliding_rp_violation\",\n",
    "        \"rp_violation\",\n",
    "        \"drift\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "metrics.to_csv(sorting_folder / \"metrics.csv\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to report\n",
    "# from spikeinterface import exporters\n",
    "_ = si.postprocessing.compute_spike_amplitudes(waveform_extractor=we)\n",
    "# _ = si.postprocessing.compute_correlograms(waveform_extractor=we)\n",
    "_ = si.qualitymetrics.compute_quality_metrics(\n",
    "    waveform_extractor=we, metric_names=[\"snr\", \"isi_violation\", \"presence_ratio\"]\n",
    ")\n",
    "\n",
    "si.exporters.export_report(we, output_folder=sorting_folder / \"report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kilosort2_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install docker\n",
    "# !pip install cuda-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorter_name = \"kilosort2\"\n",
    "sorting_folder = (\n",
    "    get_processed_root_data_dir()\n",
    "    / (key[\"organoid_id\"] + \"-\" + str(key[\"start_time\"].time()).replace(\":\", \"-\"))\n",
    "    / sorter_name\n",
    ")\n",
    "\n",
    "sorting_kilosort = si.sorters.run_sorter(\n",
    "    recording=recording_cmr,\n",
    "    sorter_name=sorter_name,\n",
    "    output_folder=sorting_folder,\n",
    "    remove_existing_folder=True,\n",
    "    verbose=True,\n",
    "    docker_image=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "organoid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
