{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected at least 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspikeinterface\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msi\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m widgets, exporters, postprocessing, qualitymetrics, sorters\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mworkflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mworkflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaths\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     get_ephys_root_data_dir,\n\u001b[1;32m     13\u001b[0m     get_raw_root_data_dir,\n\u001b[1;32m     14\u001b[0m     get_processed_root_data_dir,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/utah_organoids/src/workflow/__init__.py:26\u001b[0m\n\u001b[1;32m     21\u001b[0m dj\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_root_data_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROCESSED_ROOT_DATA_DIR\u001b[39m\u001b[38;5;124m\"\u001b[39m, dj\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_root_data_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m DB_PREFIX: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m dj\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabase.prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m ORG_NAME, WORKFLOW_NAME, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m DB_PREFIX\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m SUPPORT_DB_PREFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mORG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_support_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWORKFLOW_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m REL_PATH_INBOX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mORG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWORKFLOW_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/inbox\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected at least 2, got 1)"
     ]
    }
   ],
   "source": [
    "# Load Modulues\n",
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "import datajoint as dj\n",
    "from datetime import datetime\n",
    "import spikeinterface as si\n",
    "from spikeinterface import widgets, exporters, postprocessing, qualitymetrics, sorters\n",
    "from workflow.pipeline import *\n",
    "from workflow.utils.paths import (\n",
    "    get_ephys_root_data_dir,\n",
    "    get_raw_root_data_dir,\n",
    "    get_processed_root_data_dir,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Dictionaries\n",
    "Batches = [\"Batch 1\" , \"Batch 2\" , \"Batch 3\"]\n",
    "Drugs = [\"4-AP\" , \"No Drug\" , \"Bicuculline\" , \"Tetrodotoxin\"]\n",
    "Organoids = {\n",
    "    \"Batch 1\":[\"O09\" , \"O10\" , \"O11\" , \"O12\"],\n",
    "    \"Batch 2\":[\"O13\" , \"O14\" , \"O15\" , \"O16\"],\n",
    "    \"Batch 3\":[\"O17\" , \"O18\" , \"O19\" , \"O20\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random functions\n",
    "\n",
    "def get_dir_info(clustering_output_dir):\n",
    "    # take clustering output dir and extract batch , drug_name , organoid , start_time , end_time\n",
    "\n",
    "    # find organoid , start time , end time\n",
    "    query = ephys.ClusteringTask() & f\"clustering_output_dir = '{clustering_output_dir}'\"\n",
    "    organoid , experiment , start_time , end_time = query.fetch1(\"organoid_id\" , \"experiment_start_time\" , \"start_time\" , \"end_time\")\n",
    "\n",
    "    # find batch\n",
    "    if organoid in Organoids[\"Batch 1\"]:\n",
    "        batch = \"Batch 1\"\n",
    "    elif organoid in Organoids[\"Batch 2\"]:\n",
    "        batch = \"Batch 2\"\n",
    "    elif organoid in Organoids[\"Batch 3\"]:\n",
    "        batch = \"Batch 3\"\n",
    "    \n",
    "    # find drug name\n",
    "    drug = (culture.Experiment() & f\"organoid_id = '{organoid}' AND experiment_start_time = '{experiment}'\").fetch1(\"drug_name\")\n",
    "\n",
    "    info = {\n",
    "        \"batch\":batch,\n",
    "        \"drug\":drug,\n",
    "        \"organoid\":organoid,\n",
    "        \"start_time\":start_time,\n",
    "        \"end_time\":end_time\n",
    "        }\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data functions\n",
    "class data:\n",
    "\n",
    "    def get_tasks(global_key):\n",
    "        # get clustering tasks which will contain data to be used (time segments and organoids)\n",
    "        \n",
    "        # global key = list of dictionaries of wanted Clustering Tasks - [{AND conditions} OR {AND conditions} OR ...]\n",
    "        #   --> ex: [{\"organoid_id\":\"O09\"} , {\"organoid_id\":\"O10\" , \"start_time\":datetime(2023, 5, 18, 12, 25)}]\n",
    "        #           = all tasks with organoid O09 or tasks with organoid O10 and start time 5/18/2023 , 12:25\n",
    "        # key options: organoid_id , experiment_start_time , insertion_number , start_time , end_time , paramset_idx , clustering_output_dir\n",
    "\n",
    "        # Get string key based on global key\n",
    "        str_key = \"\" # initialize final string key\n",
    "\n",
    "        for idx_OR , key in enumerate(global_key): # loop through global key (OR statements)\n",
    "            \n",
    "            str_OR = \"\" # intialize string for keys\n",
    "\n",
    "            for idx_AND , (param , value) in enumerate(key.items()): # loop through individual keys (AND statements)\n",
    "\n",
    "                str_AND = f\"{param} = '{value}'\" # create AND conditions\n",
    "\n",
    "                if idx_AND == len(key)-1: # if the param,value pair isn't the last one (account for 0)\n",
    "                    str_OR += str_AND # add onto OR condition without AND statement\n",
    "                else:\n",
    "                    str_OR += str_AND + \" AND \" # add onto OR condition with AND statement\n",
    "                    \n",
    "            if idx_OR == len(global_key)-1: # same logic as AND statements\n",
    "                str_key += str_OR\n",
    "            else:\n",
    "                str_key += str_OR + \" OR \"\n",
    "\n",
    "        # Extract wanted tasks\n",
    "        tasks = ephys.ClusteringTask() & str_key\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "    def get_data(Tasks , global_query , Values):\n",
    "        # get data values based on clustering task\n",
    "\n",
    "        # Tasks = query of clustering tasks generated by get_tasks\n",
    "        # global_query = query where to access the data\n",
    "        # Values = wanted data values (ex: amplitude, firing rate, electrode, waveform)\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        for task in Tasks: # loop through wanted values\n",
    "\n",
    "            # get task info\n",
    "            clustering_output_dir = task[\"clustering_output_dir\"]\n",
    "            task_info = get_dir_info(clustering_output_dir=clustering_output_dir)\n",
    "            task_title = \"/\".join(map(str, task_info.values()))\n",
    "\n",
    "            # get task data query\n",
    "            task_query = global_query & f\"organoid_id='{task_info['organoid']}' AND start_time='{task_info['start_time']}' AND end_time='{task_info['end_time']}'\"\n",
    "\n",
    "            if len(task_query) == 0:\n",
    "\n",
    "                data[task_title] = \"no data\"\n",
    "\n",
    "            else:\n",
    "                \n",
    "                data[task_title] = {}\n",
    "                for value in Values: # loop through tasks\n",
    "                    \n",
    "                    # extract value data\n",
    "                    value_data = task_query.fetch(value)\n",
    "\n",
    "                    # put into dictionary\n",
    "                    data[task_title][value] = value_data\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Data functions\n",
    "\n",
    "class plot:\n",
    "\n",
    "    def get_bar(bar , errorbar , points , bargap=.2 , groupgap=.2):\n",
    "        # make bar plot\n",
    "\n",
    "        # bar = height of bars (2D numpy , each 1D are the values for a single group)\n",
    "        # errorbar = height of errorbar (2D numpy , 0 for no bar)\n",
    "        # points = any points to be plotted (2D numpy , each 1D numpy is for a group , it's a list of tuples each representing points for a bar)\n",
    "        # bargap = gap between bars (center to center)\n",
    "        # groupgap = gap between groups (center to center)\n",
    "\n",
    "        # Get X values (along x axis)\n",
    "        numgroup , numbar = bar.shape\n",
    "        xvalues = np.tile(np.arange(numbar) , (numgroup,1)) \n",
    "        groupoffset = (np.arange(numgroup)[:, np.newaxis]) * (np.ones((1, numbar)) * (groupgap + bargap*numbar))\n",
    "        xvalues += groupoffset\n",
    "\n",
    "        # Initialize Axis lists\n",
    "        bar_ax = []\n",
    "        error_ax = []\n",
    "        points_ax = []      \n",
    "\n",
    "        for i in range(numgroup):\n",
    "            \n",
    "            # Get group values\n",
    "            x_i = xvalues[i]\n",
    "            bar_i = bar[i]\n",
    "            error_i = errorbar[i]\n",
    "            points_i = points[i]\n",
    "\n",
    "            # Plot\n",
    "            bar_ax.append(plt.bar(x_i , bar_i , bargap , edgecolor='k'))\n",
    "            error_ax.append(plt.errorbar(x_i , bar_i , yerr=error_i , fmt='o' , color='k' , capsize=4 , markersize=0))\n",
    "            points_ax.append([])\n",
    "            for ii in range(numbar):\n",
    "        \n",
    "                x_ii = x_i[ii]\n",
    "                points_ii = points_i[ii]\n",
    "\n",
    "                points_ax[i].append(plt.scatter([x_ii]*len(points_ii) + (np.random.random(len(points_ii)))*(bargap/2)-(bargap/4) , points_ii))\n",
    "\n",
    "        return bar_ax , error_ax , points_ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data Example\n",
    "\n",
    "global_key = [{\"organoid_id\":\"O09\" , \"experiment_start_time\":datetime(2023, 5, 18, 12, 25)}]\n",
    "Tasks = data.get_tasks(global_key=global_key)\n",
    "\n",
    "global_query = ephys.QualityMetrics.Cluster()\n",
    "Values = [\"firing_rate\" , \"snr\"]\n",
    "\n",
    "data = data.get_data(Tasks=Tasks , global_query=global_query , Values=Values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utah_organoids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
